{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json           \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"authorization\": \"Bearer 1/1204862582358995:58225b495568b45ac37d588db2fea9a6\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asanaResToCSV(csvName, res):\n",
    "    '''\n",
    "        csvName: name of csv file in which data to dump\n",
    "        res: response of the asana api\n",
    "    '''\n",
    "    resDict = json.loads(res.text) # String to dict\n",
    "    df = pd.DataFrame.from_dict(resDict['data'])\n",
    "    if not os.path.exists('./fetchedRecords'):\n",
    "        os.makedirs('./fetchedRecords')\n",
    "    df.to_csv(f'./fetchedRecords/{csvName}.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = \"https://app.asana.com/api/1.0/workspaces\"\n",
    "resWorkspace = requests.get(workspace, headers=headers)\n",
    "# print(resWorkspace.text) # type ==> 'dict' \n",
    "asanaResToCSV('workspace', resWorkspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = \"https://app.asana.com/api/1.0/projects\"\n",
    "resProjects = requests.get(projects, headers=headers)\n",
    "# print((resProjects.text)) # type ==> 'dict' \n",
    "asanaResToCSV('projects', resProjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1204862567620455'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the project ID for fetching sections and tasks\n",
    "resProjectsDict = json.loads(resProjects.text)\n",
    "projectID = resProjectsDict['data'][0]['gid']\n",
    "projectID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = f'https://app.asana.com/api/1.0/projects/{projectID}/sections'\n",
    "resSection = requests.get(section, headers=headers)\n",
    "# print((resSection.text)) # type ==> 'dict' \n",
    "asanaResToCSV('section', resSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching all modified and unmodified tasks \n",
    "tasks = f\"https://app.asana.com/api/1.0/tasks?project={projectID}\"\n",
    "resTasks = requests.get(tasks, headers=headers)\n",
    "# print((resTasks.text)) # type ==> 'dict' \n",
    "asanaResToCSV('tasks', resTasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No modified records!!!\n"
     ]
    }
   ],
   "source": [
    "current_datetime = datetime.utcnow().isoformat()\n",
    "encoded_datetime = urllib.parse.quote(current_datetime)\n",
    "\n",
    "try: # file exists ==> some etl has already performed \n",
    "    df_tasks = pd.read_csv('./fetchedRecords/tasks.csv') # Assumed that file is always present\n",
    "    # print(f'Previous df\\n{df_tasks}\\n')    \n",
    "except FileNotFoundError: # etl running for the first time \n",
    "    df_tasks = pd.DataFrame(columns=['gid', 'name', 'resource_type', 'resource_subtype', 'etl_date_created', 'etl_date_modified'])\n",
    "    past_date = datetime(2000, 1, 1, 00, 00, 0000)\n",
    "    last_modified = past_date.strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "else:\n",
    "    last_modified = df_tasks['etl_date_modified'].max()\n",
    "\n",
    "tasks = f\"https://app.asana.com/api/1.0/tasks?project={projectID}&modified_since={last_modified}\"\n",
    "resTasks = requests.get(tasks, headers=headers) \n",
    "resDict = json.loads(resTasks.text) # String to dict\n",
    "df_modifiedTasks = pd.DataFrame.from_dict(resDict['data'])\n",
    "# print(f'fetched df\\n{df_modifiedTasks}\\n')\n",
    "    \n",
    "if len(df_modifiedTasks) == 0: # if no records are modifed since last_modified date \n",
    "    print('No modified records!!!')    \n",
    "else:\n",
    "    df_tasks['gid'] = df_tasks['gid'].astype(str)\n",
    "\n",
    "    # Adding etl_date_created and etl_date_modified columns in fetched df\n",
    "    df_modifiedTasks['etl_date_modified'] = encoded_datetime\n",
    "    df_modifiedTasks = df_modifiedTasks.merge(df_tasks[['gid', 'etl_date_created']], on='gid', how='left')\n",
    "    df_modifiedTasks['etl_date_created'].fillna(encoded_datetime, inplace=True)\n",
    "    # print(f'processed fetched df\\n{df_modifiedTasks}\\n')\n",
    "\n",
    "    # merging the two dfs\n",
    "    df_merged = pd.concat([df_tasks, df_modifiedTasks]).drop_duplicates(subset='gid', keep='last')\n",
    "    # print(f'final df\\n{df_merged}\\n')\n",
    "\n",
    "    if not os.path.exists('./fetchedRecords'):\n",
    "        os.makedirs('./fetchedRecords')        \n",
    "    df_merged.to_csv(f'./fetchedRecords/tasks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed fetched df\n",
      "                gid                           name resource_type  \\\n",
      "0  1204862567620465            Draft project brief          task   \n",
      "1  1204862567620467                      renamed 1          task   \n",
      "2  1204862567620469  Share timeline with teammates          task   \n",
      "3  1205085348231103                    test task 1          task   \n",
      "4  1205110993213486                    test task 2          task   \n",
      "\n",
      "  resource_subtype               etl_date_modified  \\\n",
      "0     default_task  2023-07-21T14%3A09%3A19.511638   \n",
      "1     default_task  2023-07-21T14%3A09%3A19.511638   \n",
      "2     default_task  2023-07-21T14%3A09%3A19.511638   \n",
      "3     default_task  2023-07-21T14%3A09%3A19.511638   \n",
      "4     default_task  2023-07-21T14%3A09%3A19.511638   \n",
      "\n",
      "                 etl_date_created  \n",
      "0  2023-07-21T12%3A01%3A18.123008  \n",
      "1  2023-07-21T12%3A01%3A18.123008  \n",
      "2  2023-07-21T12%3A01%3A18.123008  \n",
      "3  2023-07-21T12%3A01%3A18.123008  \n",
      "4  2023-07-21T12%3A47%3A06.500749  \n",
      "\n",
      "final df\n",
      "                gid                           name resource_type  \\\n",
      "0  1204862567620465            Draft project brief          task   \n",
      "1  1204862567620467                      renamed 1          task   \n",
      "2  1204862567620469  Share timeline with teammates          task   \n",
      "3  1205085348231103                    test task 1          task   \n",
      "4  1205110993213486                    test task 2          task   \n",
      "\n",
      "  resource_subtype                etl_date_created  \\\n",
      "0     default_task  2023-07-21T12%3A01%3A18.123008   \n",
      "1     default_task  2023-07-21T12%3A01%3A18.123008   \n",
      "2     default_task  2023-07-21T12%3A01%3A18.123008   \n",
      "3     default_task  2023-07-21T12%3A01%3A18.123008   \n",
      "4     default_task  2023-07-21T12%3A47%3A06.500749   \n",
      "\n",
      "                etl_date_modified actual_time_minutes  \\\n",
      "0  2023-07-21T14%3A09%3A19.511638                None   \n",
      "1  2023-07-21T14%3A09%3A19.511638                None   \n",
      "2  2023-07-21T14%3A09%3A19.511638                None   \n",
      "3  2023-07-21T14%3A09%3A19.511638                None   \n",
      "4  2023-07-21T14%3A09%3A19.511638                None   \n",
      "\n",
      "                                            assignee assignee_status  \\\n",
      "0  {'gid': '1204862582358995', 'name': 'Muhammad ...           inbox   \n",
      "1  {'gid': '1204862582358995', 'name': 'Muhammad ...           inbox   \n",
      "2                                               None        upcoming   \n",
      "3  {'gid': '1204862582358995', 'name': 'Muhammad ...           inbox   \n",
      "4                                               None        upcoming   \n",
      "\n",
      "                                    assignee_section  ... notes num_hearts  \\\n",
      "0  {'gid': '1204862567026504', 'name': 'Recently ...  ...              0.0   \n",
      "1  {'gid': '1204862567026504', 'name': 'Recently ...  ...              0.0   \n",
      "2                                                NaN  ...              0.0   \n",
      "3  {'gid': '1204862567026504', 'name': 'Recently ...  ...              0.0   \n",
      "4                                                NaN  ...              0.0   \n",
      "\n",
      "  num_likes parent                                      permalink_url  \\\n",
      "0       0.0   None  https://app.asana.com/0/1204862567620455/12048...   \n",
      "1       0.0   None  https://app.asana.com/0/1204862567620455/12048...   \n",
      "2       0.0   None  https://app.asana.com/0/1204862567620455/12048...   \n",
      "3       0.0   None  https://app.asana.com/0/1204862567620455/12050...   \n",
      "4       0.0   None  https://app.asana.com/0/1204862567620455/12051...   \n",
      "\n",
      "                                            projects start_at start_on tags  \\\n",
      "0  [{'gid': '1204862567620455', 'name': 'Cross-fu...     None     None   []   \n",
      "1  [{'gid': '1204862567620455', 'name': 'Cross-fu...     None     None   []   \n",
      "2  [{'gid': '1204862567620455', 'name': 'Cross-fu...     None     None   []   \n",
      "3  [{'gid': '1204862567620455', 'name': 'Cross-fu...     None     None   []   \n",
      "4  [{'gid': '1204862567620455', 'name': 'Cross-fu...     None     None   []   \n",
      "\n",
      "                                           workspace  \n",
      "0  {'gid': '1204862567026473', 'name': 'My worksp...  \n",
      "1  {'gid': '1204862567026473', 'name': 'My worksp...  \n",
      "2  {'gid': '1204862567026473', 'name': 'My worksp...  \n",
      "3  {'gid': '1204862567026473', 'name': 'My worksp...  \n",
      "4  {'gid': '1204862567026473', 'name': 'My worksp...  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_datetime = datetime.utcnow().isoformat()\n",
    "encoded_datetime = urllib.parse.quote(current_datetime)\n",
    "\n",
    "df_tasks = pd.read_csv('./fetchedRecords/tasks.csv')\n",
    "tasksDetails_list = []\n",
    "\n",
    "try: # file exists ==> some etl has already performed \n",
    "    df_existedTaskDetails = pd.read_csv('./fetchedRecords/taskDetails.csv') # Assumed that file is always present\n",
    "    # print(f'Previous df\\n{df_existedTaskDetails}\\n')    \n",
    "except FileNotFoundError: # etl running for the first time \n",
    "    # getting gids of all tasks \n",
    "    gids_tasksToFetch = list(df_tasks['gid'])\n",
    "else:\n",
    "    # getting the gids of tasks which are modifed after taskDetail etl previous run\n",
    "    df_taskAndTaskDetails = pd.merge(df_tasks, df_existedTaskDetails, on='gid', how = 'left', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    gids_modifiedTasks = list(df_taskAndTaskDetails[df_taskAndTaskDetails['etl_date_modified_df1'] > df_taskAndTaskDetails['etl_date_modified_df2']]['gid'])\n",
    "    gids_newTasks = list(df_taskAndTaskDetails[df_taskAndTaskDetails['etl_date_modified_df2'].isnull()]['gid'])\n",
    "\n",
    "    gids_tasksToFetch = gids_modifiedTasks + gids_newTasks  \n",
    "\n",
    "df_tasks['gid'] = df_tasks['gid'].astype(str)\n",
    "df_existedTaskDetails['gid'] = df_existedTaskDetails['gid'].astype(str)\n",
    "\n",
    "# calling api for the modified task details only\n",
    "for task_gid in gids_tasksToFetch:\n",
    "    task = f\"https://app.asana.com/api/1.0/tasks/{task_gid}\"\n",
    "    resTask = requests.get(task, headers=headers)\n",
    "    resDict = json.loads(resTask.text) # String to dict\n",
    "    taskDetails = pd.json_normalize(resDict['data'], max_level=0)\n",
    "    tasksDetails_list.append(taskDetails)\n",
    "df_newTaskDetails = pd.concat(tasksDetails_list)\n",
    "\n",
    "# Adding etl_date_created and etl_date_modified columns in fetched df\n",
    "df_newTaskDetails['etl_date_modified'] = encoded_datetime\n",
    "df_newTaskDetails = df_newTaskDetails.merge(df_existedTaskDetails[['gid', 'etl_date_created']], on='gid', how='left')\n",
    "df_newTaskDetails['etl_date_created'].fillna(encoded_datetime, inplace=True)    \n",
    "print(f'processed fetched df\\n{df_modifiedTasks.head()}\\n')\n",
    "\n",
    "# merging the two dfs\n",
    "df_updatedTaskDetails = pd.concat([df_existedTaskDetails, df_newTaskDetails]).drop_duplicates(subset='gid', keep='last')\n",
    "print(f'final df\\n{df_updatedTaskDetails.head()}\\n')\n",
    "\n",
    "if not os.path.exists('./fetchedRecords'):\n",
    "    os.makedirs('./fetchedRecords')        \n",
    "df_merged.to_csv(f'./fetchedRecords/taskDetails.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
